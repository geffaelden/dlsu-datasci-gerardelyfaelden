{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import datetime\n",
    "import csv\n",
    "import os \n",
    "import requests \n",
    "import datetime as dt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your exercise do the following:\n",
    "\n",
    "1. Choose a reddit page you want to crawl\n",
    "2. The following fields should be present when you crawl **(10 points)**:\n",
    "    - author\n",
    "    - subreddit\n",
    "    - date created \n",
    "    - number of comments \n",
    "    - score\n",
    "    - submission title \n",
    "    - submission description\n",
    "3. After crawling, save your results to a pandas dataframe **(3 points)**. \n",
    "4. Answer the following questions **(12 points)**:\n",
    "    - How many submissions were you able to gather? \n",
    "    - Who has the most submissions? \n",
    "    - Which submission has the highest score? \n",
    "    - Which submission has the highest number of comments?\n",
    "    - Which day of the week has the most submissions? \n",
    "    \n",
    "**Tip:** _For item#4, recall how to use the aggregation functions in `pandas` like count, value_counts, sum, etc. For getting the day of the week, look into how to get the `dayofweek` from a datetime object in `pandas`. (Hint: You may need to use `pd.to_datetime` to convert your date column...)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author',\n",
       " 'author_flair_css_class',\n",
       " 'author_flair_richtext',\n",
       " 'author_flair_text',\n",
       " 'author_flair_type',\n",
       " 'author_fullname',\n",
       " 'author_patreon_flair',\n",
       " 'can_mod_post',\n",
       " 'contest_mode',\n",
       " 'created_utc',\n",
       " 'domain',\n",
       " 'full_link',\n",
       " 'gildings',\n",
       " 'id',\n",
       " 'is_crosspostable',\n",
       " 'is_meta',\n",
       " 'is_original_content',\n",
       " 'is_reddit_media_domain',\n",
       " 'is_robot_indexable',\n",
       " 'is_self',\n",
       " 'is_video',\n",
       " 'link_flair_background_color',\n",
       " 'link_flair_richtext',\n",
       " 'link_flair_text_color',\n",
       " 'link_flair_type',\n",
       " 'locked',\n",
       " 'media_only',\n",
       " 'no_follow',\n",
       " 'num_comments',\n",
       " 'num_crossposts',\n",
       " 'over_18',\n",
       " 'permalink',\n",
       " 'pinned',\n",
       " 'post_hint',\n",
       " 'preview',\n",
       " 'retrieved_on',\n",
       " 'score',\n",
       " 'selftext',\n",
       " 'send_replies',\n",
       " 'spoiler',\n",
       " 'stickied',\n",
       " 'subreddit',\n",
       " 'subreddit_id',\n",
       " 'subreddit_subscribers',\n",
       " 'subreddit_type',\n",
       " 'thumbnail',\n",
       " 'thumbnail_height',\n",
       " 'thumbnail_width',\n",
       " 'title',\n",
       " 'updated_utc',\n",
       " 'url']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example from 01_Reddit_Crawler\n",
    "\n",
    "URL = \"https://api.pushshift.io/reddit/submission/search/\"  #query submissions\n",
    "PARAMS = {\n",
    "    'after': 1483228800, #get dates after March 31, 2020\n",
    "    'before': 1588291200, #get dates before May 1, 2020\n",
    "    'sort_type': 'score', # sort by score\n",
    "    'sort': 'desc', # sort in descending order\n",
    "    'subreddit': 'dlsu', # do a search on DLSU subreddit\n",
    "    'size': 20, # give only 20 search results\n",
    "#     'fields': [\"id\",\"title\",\"selftext\",\"score\",\"num_comments\",\"created_utc\"] #return only the following fields\n",
    "}\n",
    "\n",
    "#use the requests library to query pushshift api\n",
    "r = requests.get(url = URL, params=PARAMS)\n",
    "\n",
    "#get the list of available fields\n",
    "list(r.json()['data'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the API call\n",
    "url = \"https://api.pushshift.io/reddit/submission/search/\"\n",
    "subreddit = 'dlsu'\n",
    "fields = ['author', 'subreddit','created_utc','num_comments','score','title','selftext'] #required fields as per exercise instructions\n",
    "sort=\"desc\"\n",
    "\n",
    "subreddit = 'dlsu'\n",
    "url = \"https://api.pushshift.io/reddit/submission/search/\"\n",
    "results = []\n",
    "\n",
    "URL = \"https://api.pushshift.io/reddit/submission/search/\"  #query submissions\n",
    "PARAMS = {\n",
    "    'after': 1483228800, #get dates after March 31, 2020\n",
    "    'before': 1588291200, #get dates before May 1, 2020\n",
    "    'sort_type': 'score', # sort by score\n",
    "    'sort': 'desc', # sort in descending order\n",
    "    'subreddit': 'dlsu', # do a search on DLSU subreddit\n",
    "    'size': 20, # give only 20 search results\n",
    "#     'fields': [\"id\",\"title\",\"selftext\",\"score\",\"num_comments\",\"created_utc\"] #return only the following fields\n",
    "}\n",
    "\n",
    "#use the requests library to query pushshift api\n",
    "r = requests.get(url = URL, params=PARAMS)\n",
    "#parse returned data to a json object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
