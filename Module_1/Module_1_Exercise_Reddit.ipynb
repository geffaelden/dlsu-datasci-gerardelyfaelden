{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import datetime\n",
    "import csv\n",
    "import os \n",
    "import requests \n",
    "import datetime as dt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your exercise do the following:\n",
    "\n",
    "1. Choose a reddit page you want to crawl\n",
    "2. The following fields should be present when you crawl **(10 points)**:\n",
    "    - author\n",
    "    - subreddit\n",
    "    - date created \n",
    "    - number of comments \n",
    "    - score\n",
    "    - submission title \n",
    "    - submission description\n",
    "3. After crawling, save your results to a pandas dataframe **(3 points)**. \n",
    "4. Answer the following questions **(12 points)**:\n",
    "    - How many submissions were you able to gather? \n",
    "    - Who has the most submissions? \n",
    "    - Which submission has the highest score? \n",
    "    - Which submission has the highest number of comments?\n",
    "    - Which day of the week has the most submissions? \n",
    "    \n",
    "**Tip:** _For item#4, recall how to use the aggregation functions in `pandas` like count, value_counts, sum, etc. For getting the day of the week, look into how to get the `dayofweek` from a datetime object in `pandas`. (Hint: You may need to use `pd.to_datetime` to convert your date column...)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utc_to_unix(date):\n",
    "    '''Forces a timestamp into the UTC timezone and converts it to a UNIX epoch'''\n",
    "    return int(date.replace(tzinfo=dt.timezone.utc).timestamp())\n",
    "\n",
    "def unix_to_utc(unix):\n",
    "    '''Converts a UNIX epoch to a UTC Timestamp'''\n",
    "    return datetime.datetime.utcfromtimestamp(unix).strftime('%Y-%m-%d %H:%M:%S')\n",
    "#prepare the API call\n",
    "url = \"https://api.pushshift.io/reddit/submission/search/\"\n",
    "subreddit = 'ProRevenge' #/r/ProRevenge subreddit\n",
    "fields = ['author', 'subreddit','created_utc','num_comments','score','title','selftext'] #required fields as per exercise instructions\n",
    "sort_type = 'created_utc'\n",
    "sort = 'asc'\n",
    "size = 500 \n",
    "\n",
    "#Declare start and end of reddit posts to extract \n",
    "start_date = dt.datetime.strptime(\"2019-01-01\", \"%Y-%m-%d\")\n",
    "end_date = dt.datetime.strptime(\"2020-01-01\", \"%Y-%m-%d\")\n",
    "\n",
    "URL = \"https://api.pushshift.io/reddit/submission/search/\"  #query submissions\n",
    "PARAMS = {\n",
    "    'after': utc_to_unix(start_date)-1, #get dates after Jan 1, 2019 UTC\n",
    "    'before': utc_to_unix(end_date), #get dates before Jan 1, 2020 UTC\n",
    "    'sort_type': sort_type, # sort by created_utc\n",
    "    'sort': sort, # sort in descending order\n",
    "    'subreddit': subreddit, # do a search on ProRevenge subreddit\n",
    "    'size': size, # give only 500 search results\n",
    "    'fields': fields #return only the following fields\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done until 2019-01-15 10:14:02 - Result Size 100 - Total Results Size 100\n",
      "Done until 2019-01-30 00:07:07 - Result Size 100 - Total Results Size 200\n",
      "Done until 2019-02-13 15:05:33 - Result Size 100 - Total Results Size 300\n",
      "Done until 2019-02-17 09:05:14 - Result Size 100 - Total Results Size 400\n",
      "Done until 2019-02-18 19:31:47 - Result Size 100 - Total Results Size 500\n",
      "Done until 2019-02-19 23:36:57 - Result Size 100 - Total Results Size 600\n",
      "Done until 2019-02-21 02:00:10 - Result Size 100 - Total Results Size 700\n",
      "Done until 2019-02-21 23:38:23 - Result Size 100 - Total Results Size 800\n",
      "Done until 2019-02-23 04:46:35 - Result Size 100 - Total Results Size 900\n",
      "Done until 2019-02-24 03:13:30 - Result Size 100 - Total Results Size 1000\n",
      "Done until 2019-02-24 22:18:53 - Result Size 100 - Total Results Size 1100\n",
      "Done until 2019-02-25 20:38:57 - Result Size 100 - Total Results Size 1200\n",
      "Done until 2019-02-26 19:03:11 - Result Size 100 - Total Results Size 1300\n",
      "Done until 2019-02-27 18:56:43 - Result Size 100 - Total Results Size 1400\n",
      "Done until 2019-02-28 16:19:18 - Result Size 100 - Total Results Size 1500\n",
      "Done until 2019-03-01 14:29:57 - Result Size 100 - Total Results Size 1600\n",
      "Done until 2019-03-02 14:50:49 - Result Size 100 - Total Results Size 1700\n",
      "Done until 2019-03-03 08:42:22 - Result Size 100 - Total Results Size 1800\n",
      "Done until 2019-03-04 03:24:59 - Result Size 100 - Total Results Size 1900\n",
      "Done until 2019-03-04 23:50:09 - Result Size 100 - Total Results Size 2000\n",
      "Done until 2019-03-05 21:07:37 - Result Size 100 - Total Results Size 2100\n",
      "Done until 2019-03-06 19:28:09 - Result Size 100 - Total Results Size 2200\n",
      "Done until 2019-03-07 17:41:41 - Result Size 100 - Total Results Size 2300\n",
      "Done until 2019-03-08 15:30:05 - Result Size 100 - Total Results Size 2400\n",
      "Done until 2019-03-09 12:11:15 - Result Size 100 - Total Results Size 2500\n",
      "Done until 2019-03-10 09:44:25 - Result Size 100 - Total Results Size 2600\n",
      "Done until 2019-03-11 06:02:55 - Result Size 100 - Total Results Size 2700\n",
      "Done until 2019-03-11 23:10:52 - Result Size 100 - Total Results Size 2800\n",
      "Done until 2019-03-12 21:55:41 - Result Size 100 - Total Results Size 2900\n",
      "Done until 2019-03-13 23:36:35 - Result Size 100 - Total Results Size 3000\n",
      "Done until 2019-03-15 00:23:54 - Result Size 100 - Total Results Size 3100\n",
      "Done until 2019-03-16 02:00:48 - Result Size 100 - Total Results Size 3200\n",
      "Done until 2019-03-17 11:27:07 - Result Size 100 - Total Results Size 3300\n",
      "Done until 2019-03-18 16:48:33 - Result Size 100 - Total Results Size 3400\n",
      "Done until 2019-03-20 07:48:40 - Result Size 100 - Total Results Size 3500\n",
      "Done until 2019-03-21 22:24:21 - Result Size 100 - Total Results Size 3600\n",
      "Done until 2019-03-23 15:44:29 - Result Size 100 - Total Results Size 3700\n",
      "Done until 2019-03-25 11:21:49 - Result Size 100 - Total Results Size 3800\n",
      "Done until 2019-03-27 08:01:10 - Result Size 100 - Total Results Size 3900\n",
      "Done until 2019-03-29 15:24:52 - Result Size 100 - Total Results Size 4000\n",
      "Done until 2019-03-31 06:18:48 - Result Size 100 - Total Results Size 4100\n",
      "Done until 2019-04-01 21:58:44 - Result Size 100 - Total Results Size 4200\n",
      "Done until 2019-04-03 20:05:46 - Result Size 100 - Total Results Size 4300\n",
      "Done until 2019-04-05 21:29:22 - Result Size 100 - Total Results Size 4400\n",
      "Done until 2019-04-08 12:52:08 - Result Size 100 - Total Results Size 4500\n",
      "Done until 2019-04-10 19:07:10 - Result Size 100 - Total Results Size 4600\n",
      "Done until 2019-04-12 21:58:03 - Result Size 100 - Total Results Size 4700\n",
      "Done until 2019-04-15 14:40:50 - Result Size 100 - Total Results Size 4800\n",
      "Done until 2019-04-17 23:14:55 - Result Size 100 - Total Results Size 4900\n",
      "Done until 2019-04-20 11:13:19 - Result Size 100 - Total Results Size 5000\n",
      "Done until 2019-04-23 02:31:43 - Result Size 100 - Total Results Size 5100\n",
      "Done until 2019-04-26 00:34:07 - Result Size 100 - Total Results Size 5200\n",
      "Done until 2019-04-28 22:04:28 - Result Size 100 - Total Results Size 5300\n",
      "Done until 2019-05-01 21:17:06 - Result Size 100 - Total Results Size 5400\n",
      "Done until 2019-05-03 23:36:34 - Result Size 100 - Total Results Size 5500\n",
      "Done until 2019-05-06 20:54:59 - Result Size 100 - Total Results Size 5600\n",
      "Done until 2019-05-10 01:34:02 - Result Size 100 - Total Results Size 5700\n",
      "Done until 2019-05-13 04:14:57 - Result Size 100 - Total Results Size 5800\n",
      "Done until 2019-05-16 13:52:27 - Result Size 100 - Total Results Size 5900\n",
      "Done until 2019-05-19 14:59:39 - Result Size 100 - Total Results Size 6000\n",
      "Done until 2019-05-22 21:04:30 - Result Size 100 - Total Results Size 6100\n",
      "Done until 2019-05-25 17:58:29 - Result Size 100 - Total Results Size 6200\n",
      "Done until 2019-05-29 06:00:38 - Result Size 100 - Total Results Size 6300\n",
      "Done until 2019-06-01 07:24:47 - Result Size 100 - Total Results Size 6400\n",
      "Done until 2019-06-04 14:44:28 - Result Size 100 - Total Results Size 6500\n",
      "Done until 2019-06-07 05:33:36 - Result Size 100 - Total Results Size 6600\n",
      "Done until 2019-06-10 16:23:55 - Result Size 100 - Total Results Size 6700\n",
      "Done until 2019-06-13 05:36:03 - Result Size 100 - Total Results Size 6800\n",
      "Done until 2019-06-16 08:06:25 - Result Size 100 - Total Results Size 6900\n",
      "Done until 2019-06-18 19:52:51 - Result Size 100 - Total Results Size 7000\n",
      "Done until 2019-06-21 13:51:55 - Result Size 100 - Total Results Size 7100\n",
      "Done until 2019-06-24 11:36:25 - Result Size 100 - Total Results Size 7200\n",
      "Done until 2019-06-27 05:08:15 - Result Size 100 - Total Results Size 7300\n",
      "Done until 2019-06-30 16:29:38 - Result Size 100 - Total Results Size 7400\n",
      "Done until 2019-07-03 22:11:21 - Result Size 100 - Total Results Size 7500\n",
      "Done until 2019-07-08 21:49:38 - Result Size 100 - Total Results Size 7600\n",
      "Done until 2019-07-12 15:11:14 - Result Size 100 - Total Results Size 7700\n",
      "Done until 2019-07-17 03:06:41 - Result Size 100 - Total Results Size 7800\n",
      "Done until 2019-07-21 01:27:37 - Result Size 100 - Total Results Size 7900\n",
      "Done until 2019-07-25 20:27:41 - Result Size 100 - Total Results Size 8000\n",
      "Done until 2019-07-29 22:45:16 - Result Size 100 - Total Results Size 8100\n",
      "Done until 2019-08-03 16:51:18 - Result Size 100 - Total Results Size 8200\n",
      "Done until 2019-08-08 00:22:06 - Result Size 100 - Total Results Size 8300\n",
      "Done until 2019-08-11 21:18:05 - Result Size 100 - Total Results Size 8400\n",
      "Done until 2019-08-15 18:21:11 - Result Size 100 - Total Results Size 8500\n",
      "Done until 2019-08-19 18:55:40 - Result Size 100 - Total Results Size 8600\n",
      "Done until 2019-08-24 03:17:14 - Result Size 100 - Total Results Size 8700\n",
      "Done until 2019-08-28 06:42:46 - Result Size 100 - Total Results Size 8800\n",
      "Done until 2019-08-31 12:32:30 - Result Size 100 - Total Results Size 8900\n",
      "Done until 2019-09-04 17:52:06 - Result Size 100 - Total Results Size 9000\n",
      "Done until 2019-09-09 19:59:18 - Result Size 100 - Total Results Size 9100\n",
      "Done until 2019-09-14 21:20:06 - Result Size 100 - Total Results Size 9200\n",
      "Done until 2019-09-19 06:48:11 - Result Size 100 - Total Results Size 9300\n",
      "Done until 2019-09-24 14:46:17 - Result Size 100 - Total Results Size 9400\n",
      "Done until 2019-09-30 22:17:17 - Result Size 100 - Total Results Size 9500\n",
      "Done until 2019-10-08 14:29:23 - Result Size 100 - Total Results Size 9600\n",
      "Done until 2019-10-16 18:53:28 - Result Size 100 - Total Results Size 9700\n",
      "Done until 2019-10-21 14:32:49 - Result Size 100 - Total Results Size 9800\n",
      "Done until 2019-10-25 13:32:17 - Result Size 100 - Total Results Size 9900\n",
      "Done until 2019-10-31 02:32:07 - Result Size 100 - Total Results Size 10000\n",
      "Done until 2019-11-07 14:47:25 - Result Size 100 - Total Results Size 10100\n",
      "Done until 2019-11-15 17:05:52 - Result Size 100 - Total Results Size 10200\n",
      "Done until 2019-11-23 22:06:50 - Result Size 100 - Total Results Size 10300\n",
      "Done until 2019-11-30 07:39:38 - Result Size 100 - Total Results Size 10400\n",
      "Done until 2019-12-07 16:07:41 - Result Size 100 - Total Results Size 10500\n",
      "Done until 2019-12-14 22:34:05 - Result Size 100 - Total Results Size 10600\n",
      "Done until 2019-12-21 14:50:17 - Result Size 100 - Total Results Size 10700\n",
      "Done until 2019-12-26 22:57:00 - Result Size 100 - Total Results Size 10800\n",
      "Done until 2019-12-31 23:09:11 - Result Size 75 - Total Results Size 10875\n"
     ]
    }
   ],
   "source": [
    "#Setup blank list\n",
    "results = []\n",
    "\n",
    "#loop while date range not fulfilled\n",
    "while PARAMS['after'] < PARAMS['before']:\n",
    "    #use the requests library to query pushshift api\n",
    "    r = requests.get(url = URL, params = PARAMS)\n",
    "    \n",
    "    if r.json()['data'] == []:\n",
    "        break\n",
    "        \n",
    "    #extend list results\n",
    "    results.extend(r.json()['data'])\n",
    "\n",
    "    #change start_time\n",
    "    start_date = r.json()['data'][-1]['created_utc'] # this sets new start time to the last timestamp in the result array\n",
    "    PARAMS['after'] = start_date\n",
    "    print('Done until {} - Result Size {} - Total Results Size {}'.format(unix_to_utc(start_date), len(r.json()['data']), len(results)))\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizes results json into a dataframe\n",
    "df = pd.json_normalize(results)\n",
    "#saves the dataframe for sanity\n",
    "df.to_csv('reddit_scrape.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pijgerk</td>\n",
       "      <td>1546308387</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>I couldn't stand Justin. My father did not lik...</td>\n",
       "      <td>ProRevenge</td>\n",
       "      <td>Ruin most of my childhood by bullying me with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sharknadotornado</td>\n",
       "      <td>1546312716</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>There is so much to this story, but I'll try t...</td>\n",
       "      <td>ProRevenge</td>\n",
       "      <td>Neighbor makes the mistake of believing he can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shablagoo-</td>\n",
       "      <td>1546353771</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/...</td>\n",
       "      <td>ProRevenge</td>\n",
       "      <td>Redditor tries to file a complaint against a p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>snackerjack7331</td>\n",
       "      <td>1546354687</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>This is an x-post from r/confession, a few peo...</td>\n",
       "      <td>ProRevenge</td>\n",
       "      <td>I scrubbed the toilet with my stepmom’s toothb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TheMightyAddicted</td>\n",
       "      <td>1546361231</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>So this history is not pro-revenge, but i thin...</td>\n",
       "      <td>ProRevenge</td>\n",
       "      <td>Trying to get high as a kite? Enjoy the trip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author  created_utc  num_comments  score  \\\n",
       "0            pijgerk   1546308387            22      1   \n",
       "1   sharknadotornado   1546312716            11      1   \n",
       "2         Shablagoo-   1546353771             3      1   \n",
       "3    snackerjack7331   1546354687            68      1   \n",
       "4  TheMightyAddicted   1546361231            55      1   \n",
       "\n",
       "                                            selftext   subreddit  \\\n",
       "0  I couldn't stand Justin. My father did not lik...  ProRevenge   \n",
       "1  There is so much to this story, but I'll try t...  ProRevenge   \n",
       "2  https://www.reddit.com/r/legaladvice/comments/...  ProRevenge   \n",
       "3  This is an x-post from r/confession, a few peo...  ProRevenge   \n",
       "4  So this history is not pro-revenge, but i thin...  ProRevenge   \n",
       "\n",
       "                                               title  \n",
       "0  Ruin most of my childhood by bullying me with ...  \n",
       "1  Neighbor makes the mistake of believing he can...  \n",
       "2  Redditor tries to file a complaint against a p...  \n",
       "3  I scrubbed the toilet with my stepmom’s toothb...  \n",
       "4       Trying to get high as a kite? Enjoy the trip  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is here because I reopened data at a different time\n",
    "df = pd.read_csv('reddit_scrape.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the following questions (12 points):\n",
    "* How many submissions were you able to gather?\n",
    "* Who has the most submissions?\n",
    "* Which submission has the highest score?\n",
    "* Which submission has the highest number of comments?\n",
    "* Which day of the week has the most submissions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 2019 Submissions: 10875\n"
     ]
    }
   ],
   "source": [
    "# How many submissions were you able to gather?\n",
    "total_submissions = len(df)\n",
    "print('Total 2019 Submissions: {}'.format(total_submissions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author with most submissions in 2019: Ford456fgfd\n"
     ]
    }
   ],
   "source": [
    "# Who has the most submissions?\n",
    "most_active = df['author'].value_counts().reset_index()[0:2]\n",
    "print('Author with most submissions in 2019: {}'.format(most_active['index'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top submission of 2019 in the ProRevenge subreddit is: \n",
      "\n",
      "Coworker tried to get me fired over breast implants, so I pulled a reverse uno card. by 3240278189\n"
     ]
    }
   ],
   "source": [
    "# Which submission has the highest score?\n",
    "top_submission = df.sort_values(by=['score'],ascending=False,ignore_index=True).head(1)\n",
    "print('The top submission of 2019 in the ProRevenge subreddit is: \\n\\n{} by {}'.format(top_submission['title'][0],top_submission['author'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top submission of 2019 in the ProRevenge subreddit is: \n",
      "\n",
      "Boyfriend of 5 years cheated on me so I ruined his precious RuneScape account by osrsbitch19\n"
     ]
    }
   ],
   "source": [
    "#Which submission has the highest number of comments?\n",
    "top_commented = df.sort_values(by=['num_comments'],ascending=False,ignore_index=True).head(1)\n",
    "print('The top submission of 2019 in the ProRevenge subreddit is: \\n\\n{} by {}'.format(top_commented['title'][0],top_commented['author'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most active day in 2019: Monday\n"
     ]
    }
   ],
   "source": [
    "# Which day of the week has the most submissions?\n",
    "df['dayname'] = pd.to_datetime(df['created_utc'], unit='s').dt.day_name() #creates dayname column\n",
    "most_active_day = df['dayname'].value_counts().reset_index()[0:2]\n",
    "print('Most active day in 2019: {}'.format(most_active_day['index'][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
